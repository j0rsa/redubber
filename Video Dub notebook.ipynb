{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c72cce5a",
   "metadata": {},
   "source": [
    "# Load and analyze video file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f5b13bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "68.78s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ffmpeg-python\n",
      "  Downloading ffmpeg_python-0.2.0-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting future (from ffmpeg-python)\n",
      "  Downloading future-1.0.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Downloading ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\n",
      "Downloading future-1.0.0-py3-none-any.whl (491 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.3/491.3 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: future, ffmpeg-python\n",
      "Successfully installed ffmpeg-python-0.2.0 future-1.0.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install ffmpeg-python openai --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5a897e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ffmpeg\n",
    "import math\n",
    "from openai import OpenAI\n",
    "\n",
    "class Redubber:\n",
    "    supported_video_formats = [\".mp4\", \".mkv\", \".avi\", \".mov\", \".flv\", \".wmv\", \".webm\", \".vob\", \".m4v\", \".3gp\", \".3g2\", \".m2ts\", \".mts\", \".ts\", \".f4v\", \".f4p\", \".f4a\", \".f4b\", \".m2v\", \".m4v\", \".m1v\", \".mpg\", \".mpeg\", \".mpv\", \".mp2\", \".mpe\", \".m2p\", \".m2t\", \".mp2v\", \".mpv2\", \".m2ts\", \".m2ts\", \".mts\", \".m2v\"]\n",
    "    tmp = \"redubber_tmp\"\n",
    "    audio_ext = \".mp3\"\n",
    "    model=\"gpt-4o\"\n",
    "    openai_token=\"\"\n",
    "    default_audio_chunk_duration = 20*60 # 20 minutes\n",
    "    \n",
    "    def can_redub(self, source):\n",
    "        return os.path.splitext(source)[1] in self.supported_video_formats\n",
    "    \n",
    "    def get_media_duration(self, file_path) -> float:\n",
    "        return float(ffmpeg.probe(file_path)['format']['duration'])\n",
    "\n",
    "    def seconds_to_hms(self, seconds):\n",
    "        hours = int(seconds // 3600)\n",
    "        minutes = int((seconds % 3600) // 60)\n",
    "        seconds = int(seconds % 60)\n",
    "        return f\"{hours:02}:{minutes:02}:{seconds:02}\"\n",
    "\n",
    "    def extract_audio_chunks(self, root, file_path, chunk_duration = default_audio_chunk_duration) -> list[str]:\n",
    "        directory_from_source = os.path.relpath(file_path, root)\n",
    "        print(f\"Extracting audio from {directory_from_source}\")\n",
    "        target_rel_dir = os.path.join(self.tmp, directory_from_source)\n",
    "        total_duration = self.get_media_duration(file_path)\n",
    "        print(f\"Video duration {self.seconds_to_hms(total_duration)}\")\n",
    "        num_chunks = math.ceil(total_duration / chunk_duration)\n",
    "        print(f\"Extracting {num_chunks} chunks of {self.seconds_to_hms(chunk_duration)} each\")\n",
    "        \n",
    "        audio_file_template = os.path.splitext(os.path.basename(file_path))[0] + \"_{:03d}\" + self.audio_ext\n",
    "        # delete all aac files in the directory\n",
    "        for root, _dirs, files in os.walk(target_rel_dir):\n",
    "            for file in files:\n",
    "                if file.endswith(self.audio_ext):\n",
    "                    os.remove(os.path.join(root, file))\n",
    "\n",
    "        total_audio_duration = 0\n",
    "        result = []\n",
    "        for i in range(num_chunks):\n",
    "            start_time = i * chunk_duration\n",
    "            output_audio_path = audio_file_template.format(i+1)  # Naming each chunk\n",
    "            audio_path = os.path.join(target_rel_dir, output_audio_path)\n",
    "            os.makedirs(target_rel_dir, exist_ok=True)\n",
    "            stream = ffmpeg.input(file_path, ss=start_time, t=chunk_duration)\n",
    "            stream = ffmpeg.output(stream, audio_path, loglevel=\"quiet\", vn=None)\n",
    "            ffmpeg.run(stream)\n",
    "\n",
    "            print(f'Extracted chunk {i+1}: {output_audio_path}')\n",
    "            result.append(audio_path)\n",
    "            total_audio_duration += self.get_media_duration(audio_path)\n",
    "        \n",
    "        print(f\"Audio duration {self.seconds_to_hms(total_audio_duration)}\")\n",
    "\n",
    "        return result\n",
    "    \n",
    "    def transcribe_audio(self, file_path, time_offset=0):\n",
    "        client = OpenAI(api_key=self.openai_token)\n",
    "        # https://platform.openai.com/docs/api-reference/audio/verbose-json-object\n",
    "        with open(file_path, \"rb\") as audio_file:\n",
    "            # Transcribe the audio using the Whisper API\n",
    "            transcript = client.audio.translations.create(model=\"whisper-1\", file=audio_file, response_format='verbose_json')\n",
    "            # print(transcript)\n",
    "            # transcript = json.loads(transcript)\n",
    "\n",
    "        segments = transcript.segments\n",
    "        for segment in segments:\n",
    "            segment['start'] += time_offset\n",
    "            segment['end'] += time_offset\n",
    "            # drop dict keys: id, tokens, seek ...\n",
    "            droppped_keys = [ 'id', 'tokens', 'seek', 'temperature', 'avg_logprob', 'compression_ratio', 'no_speech_prob']\n",
    "            for key in droppped_keys:\n",
    "                if key in segment:\n",
    "                    del segment[key]\n",
    "        \n",
    "        return transcript.text, segments\n",
    "    \n",
    "    def time_to_srt_format(self, seconds):\n",
    "        hours = int(seconds // 3600)\n",
    "        minutes = int((seconds % 3600) // 60)\n",
    "        seconds = int(seconds % 60)\n",
    "        milliseconds = int((seconds % 1) * 1000)\n",
    "        return f\"{hours:02}:{minutes:02}:{seconds:02},{milliseconds:03}\"\n",
    "\n",
    "    def write_srt(self, segments, output_file):\n",
    "        with open(output_file, 'w') as srt_file:\n",
    "            for i, segment in enumerate(segments):\n",
    "                start_time = segment['start']\n",
    "                end_time = segment['end']\n",
    "                text = segment['text']\n",
    "\n",
    "                # Convert time to SRT time format\n",
    "                start_time_str = self.time_to_srt_format(start_time)\n",
    "                end_time_str = self.time_to_srt_format(end_time)\n",
    "\n",
    "                # Write to the file\n",
    "                srt_file.write(f\"{i + 1}\\n\")\n",
    "                srt_file.write(f\"{start_time_str} --> {end_time_str}\\n\")\n",
    "                srt_file.write(f\"{text}\\n\\n\")\n",
    "\n",
    "    def tts(self, text, output_file):\n",
    "        client = OpenAI(api_key=self.openai_token)\n",
    "        with client.audio.speech.with_streaming_response.create(\n",
    "                    model=\"tts-1\",\n",
    "                    voice=\"nova\",\n",
    "                    input=text,\n",
    "                ) as response:\n",
    "            response.stream_to_file(output_file)\n",
    "\n",
    "    def tts_segments(self, segments, output_dir):\n",
    "        result = {}\n",
    "        for i, segment in enumerate(segments):\n",
    "            self.tts(segment['text'], os.path.join(output_dir, f\"{i:03d}.en.mp3\"))\n",
    "            result[segment['start']] = f\"{i:03d}.en.mp3\"\n",
    "        return result\n",
    "\n",
    "    def assemble_audio(self, audio_dict, dir, output_file, duration):\n",
    "        input_streams = []\n",
    "        for start_time, input_file in sorted(audio_dict.items()):\n",
    "            stream = ffmpeg.input(os.path.join(dir, input_file))\n",
    "            stream = stream.filter('adelay', f'{int(start_time*1000)}|{int(start_time*1000)}')\n",
    "            input_streams.append(stream)\n",
    "        \n",
    "        # Combine all input streams using amix\n",
    "        combined = ffmpeg.filter(input_streams, 'amix', inputs=len(input_streams), normalize=1)\n",
    "\n",
    "        # Apply volume boost if needed\n",
    "        combined = combined.filter('volume', len(input_streams))\n",
    "\n",
    "        # Ensure the output is of the specified duration using `apad` to pad the audio if needed\n",
    "        combined = combined.filter('atrim', end=duration).filter('apad', whole_dur=duration)\n",
    "        \n",
    "        out = ffmpeg.output(combined, output_file, acodec='libmp3lame', loglevel=\"quiet\", audio_bitrate='320k', ar='44100')\n",
    "        out = out.global_args('-y')  # Add the overwrite option\n",
    "\n",
    "        ffmpeg.run(out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f1dc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tsting with samples\n",
    "\n",
    "# stream = ffmpeg.input(\"redubber_tmp/SECTION 06. Create an attractive character face/25. Retopology.mp4/25. Retopology_001.mp3\", t=30)\n",
    "# stream = ffmpeg.output(stream, \"redubber_tmp/SECTION 06. Create an attractive character face/25. Retopology.mp4/25. Retopology_000_sample.mp3\", loglevel=\"quiet\", vn=None)\n",
    "# ffmpeg.run(stream)\n",
    "\n",
    "# sample = \"redubber_tmp/SECTION 06. Create an attractive character face/25. Retopology.mp4/25. Retopology_000_sample.mp3\"\n",
    "# text,segments = redubber.transcribe_audio(sample)\n",
    "# res = redubber.tts_segments(segments, \"redubber_tmp/SECTION 06. Create an attractive character face/25. Retopology.mp4\")\n",
    "# print(res)\n",
    "# res = {0.0: '000.en.mp3', 13.84000015258789: '001.en.mp3', 19.600000381469727: '002.en.mp3', 24.559999465942383: '003.en.mp3', 28.31999969482422: '004.en.mp3'}\n",
    "# redubber.assemble_audio(res, \"redubber_tmp/SECTION 06. Create an attractive character face/25. Retopology.mp4\", \"redubber_tmp/SECTION 06. Create an attractive character face/25. Retopology.mp4/25. Retopology_000_sample.en.mp3\", 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efc40a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "source = \"src\"\n",
    "target = \"dest\"\n",
    "\n",
    "redubber = Redubber()\n",
    "for root, dirs, files in os.walk(source):\n",
    "    for file in files:\n",
    "        src_file = os.path.join(root, file)\n",
    "        if redubber.can_redub(src_file) and '25' in src_file:\n",
    "            print(src_file)\n",
    "            audio_files = redubber.extract_audio_chunks(source, src_file)\n",
    "            \n",
    "            all_segments = []\n",
    "            for audio_file in audio_files:\n",
    "                _text, segments = redubber.transcribe_audio(audio_file)\n",
    "                all_segments.extend(segments)\n",
    "            redubber.write_srt(all_segments, os.path.join(target, os.path.splitext(os.path.basename(src_file))[0] + \".en.srt\"))\n",
    "            redubber.tts_segments(all_segments, os.path.join(target, os.path.splitext(os.path.basename(src_file))[0]))\n",
    "            redubbed_audio_path = os.path.join(target, os.path.splitext(os.path.basename(src_file))[0] + \".en.mp3\")\n",
    "            redubber.assemble_audio(all_segments, os.path.join(target, os.path.splitext(os.path.basename(src_file))[0]), redubbed_audio_path, redubber.get_media_duration(src_file))\n",
    "            # mix audio with video and save to target\n",
    "            # copy subs to target\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p313",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
